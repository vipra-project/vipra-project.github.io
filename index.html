<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ViPRA is a simple pretraining and fine-tuning framework that learns continuous robot control from actionless videos">
  <meta name="keywords" content="ViPRA, Video Prediction, Robot Actions, Robotics, Machine Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="ViPRA: Video Prediction for Robot Actions">
  <title>ViPRA: Video Prediction for Robot Actions</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vipra_robot_icon.png">
  <link rel="apple-touch-icon" href="./static/images/vipra_robot_icon.png">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-fullhd"></div>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> ViPRA: Video Prediction for Robot Actions</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sroutray.github.io">Sandeep Routray</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://hengkaipan.github.io">Hengkai Pan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://unnat.github.io">Unnat Jain</a><sup>2,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://shikharbahl.github.io">Shikhar Bahl</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~dpathak/">Deepak Pathak</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Carnegie Mellon University,</span>
            <span class="author-block"><sup>2</sup>Skild AI,</span>
            <span class="author-block"><sup>3</sup>University of California, Irvine</span>
          </div>

            <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
              <a href="static/vipra_preprint.pdf"
                 target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
              </span>
              <span class="link-block">
                <a href="https://vipra-project.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                <!-- Twitter Link. -->
                <span class="link-block">
                <a href="https://vipra-project.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Summary</span>
                </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                <a href="https://github.com/sroutray/vipra"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>
                <!-- Models Link. -->
                <span class="link-block">
                <a href="https://huggingface.co/vipra-project"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-cube"></i>
                  </span>
                  <span>Hugging Face</span>
                  </a>
                </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-fullhd">
    <div class="hero-body has-text-centered">
      <img src="./static/images/teaser_web_v3.png" style="max-width: 100%;" alt="ViPRA teaser image">
      <h2 class="subtitle mt-4">
        <!-- ViPRA learns robot control by predicting latent video dynamics from actionless demonstrations. -->
        We present ViPRA, a recipe for training generalist policy from human and robot videos via large-scale video models.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-fullhd">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present <b>V</b>ideo <b>P</b>rediction for <b>R</b>obot <b>A</b>ctions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict <i>both future visual observations and motion-centric latent actions</i>, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked <i>flow-matching decoder</i> that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, ViPRA explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

    <section class="section">
      <div class="container is-max-fullhd">
        <!-- Method Section -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <h2 class="title is-3">Method</h2>
            <figure>
              <img src="./static/images/vipra_method_web_revised.png" alt="ViPRA Method Diagram" style="max-width: 100%;">
              <figcaption class="has-text-justified mt-2 has-text-grey">
                <b>Overview of ViPRA:</b> Given human and robot videos without action labels, ViPRA learns to represent scene dynamics using a discrete latent action space. A neural quantization bottleneck extracts motion-centric latent actions from image sequences, and a video-language model is pretrained to jointly predict future observations <i>o<sub>t+H</sub></i> and latent actions <i>z<sub>t:t+H-1</sub></i> from past frames <i>(o<sub>t-1</sub>, o<sub>t</sub>)</i> and a task description <i>c</i>. The model is then finetuned on a small set of action-labeled robot demonstrations using flow matching to predict continuous action chunks <i>a<sub>t:t+H-1</sub></i>.
              </figcaption>
            </figure>
          </div>
        </div>
    
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <p>
                ViPRA is a scalable pretraining-finetuning framework that adapts a video-language model into robot control policies without requiring any action labels during pretraining. It begins by extracting fine motion-centric <i>latent actions</i>: discrete, temporally grounded representations of motion learned via a neural quantization bottleneck. These latent actions are trained using reconstruction, perceptual and optical flow consistency losses to capture meaningful scene dynamics.
              </p>
              <p>
                A video-language model is then pretrained to jointly predict future image observations and these latent actions conditioned on past image history and a natural language task prompt. This enables the model to reason about both <i>what changes</i> in the scene and <i>how</i> it changes, without ever observing real robot actions.
              </p>
              <p>
                Finally, we finetune the model on a small number of real-world robot demonstrations using a <i>flow-matching decoder</i> that maps latent actions to continuous action chunks to enable high-frequency reactive control.
              </p>
            </div>
          </div>
        </div>
    

  </div>
</section>

<section class="section">
  <div class="container is-max-fullhd">
    <h2 class="title is-3 has-text-centered">Motion-Centric Latent Actions</h2>
    <p class="content has-text-justified mb-5">
      <!-- ViPRA's latent action tokenizer encodes transferable motion semantics directly from video. By injecting latents from one clip into another, we can alter the motion while preserving scene content — for example, turning a downward trajectory into upward movement, or flipping rightward motion to the left. These cross-video transfers demonstrate that ViPRA's latent codes capture meaningful, dynamics-aware representations rather than just appearance. -->
      ViPRA's latent action tokenizer learns <em>motion-centric dynamics</em> by encoding transferable motion semantics directly from video, thus capturing how objects move in a scene. Injecting latents from one clip into another can therefore change the motion while keeping the scene intact—for instance, turning a downward trajectory into upward movement, or reversing rightward motion to the left. These cross-video transfers highlight that ViPRA's latent codes reflect meaningful, dynamics-aware behavior.
    </p>
  </div>

  <div class="columns is-variable is-6">
    <!-- Column A: Down -> Up -->
    <div class="column">
      <div class="box">
        <h4 class="title is-5 has-text-centered">Move Down → Move Up Transfer</h4>
        <figure class="image" style="margin-bottom:0.75rem;">
          <img src="./static/up_down/98681_7.gif" 
              alt="Transfer example 1: down to up" 
              loading="lazy" 
              style="max-width:100%; height:auto; border-radius:6px;">
        </figure>
        <figure class="image">
          <img src="./static/up_down/130207_4.gif" 
              alt="Transfer example 2: down to up" 
              loading="lazy" 
              style="max-width:100%; height:auto; border-radius:6px;">
        </figure>
        <p class="help has-text-centered" style="margin-top:0.5rem;">
          Upward-motion latents flips a downward-moving clip into upward motion.
        </p>
      </div>
    </div>

    <!-- Column B: Right -> Left -->
    <div class="column">
      <div class="box">
        <h4 class="title is-5 has-text-centered">Move Right → Move Left Transfer</h4>
        <figure class="image" style="margin-bottom:0.75rem;">
          <img src="./static/left_right/203562_16.gif" 
              alt="Transfer example 1: right to left" 
              loading="lazy" 
              style="max-width:100%; height:auto; border-radius:6px;">
        </figure>
        <figure class="image">
          <img src="./static/left_right/150362_9.gif" 
              alt="Transfer example 2: right to left" 
              loading="lazy" 
              style="max-width:100%; height:auto; border-radius:6px;">
        </figure>
        <p class="help has-text-centered" style="margin-top:0.5rem;">
          Leftward-motion latents flips a rightward-moving clip into leftward motion.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-fullhd">
    <h2 class="title is-3 has-text-centered">Real-World Results</h2>

    <!-- Summary Paragraph -->
    <div class="content has-text-justified mb-5">
      <p>
        We evaluate ViPRA on a Franka Panda robot across diverse real-world manipulation tasks, including pick and place, covering, and stacking. Despite being trained with limited action-labeled data, it performs smooth and reliable behaviors, often retrying failed grasps. ViPRA generalizes to novel objects and visual variations, demonstrating robustness beyond the training distribution.
      </p>      
    </div>

    <!-- Placeholder for Table or Plot -->
    <div class="has-text-centered mb-6">
      <img src="./static/images/real_bar.png" alt="Real-World Results Table" style="max-width: 100%; border-radius: 8px;">
      <figcaption class="has-text-grey has-text-justified mt-2">
        <b>Real-World results:</b> Full and partial success rates across real-world manipulation benchmarks. We compare ViPRA-FM with Pi-0, a popular vision-language-action model trained on large-scale action-labeled data, and Scratch-FM, which uses the video model directly without any pretraining.
      </figcaption>     
    </div>

    <!-- Generalization Subsection -->
    <div class="content mb-4">
      <h3 class="title is-4">Generalization to Unseen Objects</h3>
      <p>
        ViPRA generalizes to novel objects and visual variations, despite never observing them during pretraining or finetuning.
        <!-- Below, we show the robot completing a “Cover-Object” task with both seen and unseen items. -->
      </p>
    </div>

    <!-- Side-by-Side Seen vs Unseen Videos -->
    <!-- <div class="columns is-centered">
      <div class="column has-text-centered">
        <h4 class="title is-5">Seen Objects</h4>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/cover_mid_apple.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <br>
        <video controls muted autoplay loop playsinline style="max-width: 100%; margin-top: 1rem; border-radius: 8px;">
          <source src="./static/videos/cover_bowl_left.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <br>
        <video controls muted autoplay loop playsinline style="max-width: 100%; margin-top: 1rem; border-radius: 8px;">
          <source src="./static/videos/stack_cup_red_blue.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="title is-5">Unseen Objects</h4>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/cover_mid_baseball.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <br>
        <video controls muted autoplay loop playsinline style="max-width: 100%; margin-top: 1rem; border-radius: 8px;">
          <source src="./static/videos/cover_pepper_left.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
        <br>
        <video controls muted autoplay loop playsinline style="max-width: 100%; margin-top: 1rem; border-radius: 8px;">
          <source src="./static/videos/stack_cup_red_black.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div> -->

    <!-- Seen vs Unseen in Rows, Videos Across Columns -->
    <div class="container">
      <!-- Seen Row -->
      <h4 class="title is-5 has-text-centered">Seen Objects</h4>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h6 class="title is-6">Cover-Apple</h6>
          <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
            <source src="./static/videos/cover_mid_apple.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column has-text-centered">
          <h6 class="title is-6">Cover-Bowl</h6>
          <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
            <source src="./static/videos/cover_bowl_left.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column has-text-centered">
          <h6 class="title is-6">Stack-Red-on-Blue</h6>
          <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
            <source src="./static/videos/stack_cup_red_blue.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <!-- Unseen Row -->
      <h4 class="title is-5 has-text-centered">Unseen Objects</h4>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h6 class="title is-6">Cover-Baseball</h6>
          <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
            <source src="./static/videos/cover_mid_baseball.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column has-text-centered">
          <h6 class="title is-6">Cover-Pepper</h6>
          <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
            <source src="./static/videos/cover_pepper_left.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column has-text-centered">
          <h6 class="title is-6">Stack-Red-on-Black</h6>
          <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
            <source src="./static/videos/stack_cup_red_black.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    
    <!-- Robustness Subsection -->
    <div class="content mb-4 mt-6">
      <h3 class="title is-4">Robustness</h3>
      <p>
      ViPRA shows robust recovery behavior. It can retry failed grasps and adapt to external perturbations, even when objects are moved during execution.
      </p>
    </div>


    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h6 class="title is-6">Retries Failed Pick</h6>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/retry_cup_v3.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h6 class="title is-6">Adapts to External Perturbation</h6>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/pick_place_external.mp4" type="video/mp4">
        </video>
      </div>
      <div class="column has-text-centered">
        <h6 class="title is-6">Retries Failed Grasp</h6>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/retry_cover.mp4" type="video/mp4">
        </video>
      </div>
    </div>

    <!-- High-Frequency Subsection -->
    <div class="content mb-4 mt-6">
      <h3 class="title is-4">High-Frequency Control</h3>
      <p>
      ViPRA uses action chunking with a flow-matching decoder to generate smooth, continuous action sequences. It reduces inference lag with KV caching to support high-frequency reactive robot control at up to 20 Hz with 14-action chunk length.
      </p>
    </div>

    <!-- Side-by-side Videos -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h4 class="title is-5">Without KV cache (3.5Hz control)</h4>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/cover_apple_no_kv.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="title is-5">With KV cache (7Hz control)</h4>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/cover_apple_kv.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>

  </div>
</section>



<section class="section">
  <div class="container is-max-fullhd">
    <h2 class="title is-3 has-text-centered">Challenging Bimanual Tasks</h2>

    <!-- Summary Paragraph -->
    <div class="content has-text-justified mb-5">
      <p>
        Bimanual tasks require precise coordination across both arms, making them significantly harder than single-arm manipulation. ViPRA controls all 14 degrees of freedom using a single vision-language policy, executing smooth, synchronized behaviors from just a monocular camera and task prompt.
      </p>
    </div>

    <!-- Side-by-side Videos -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h4 class="title is-5">Place-in-Bowl</h4>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/bimanual_tomato_bowl.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="column has-text-centered">
        <h4 class="title is-5">Mix-with-Whisk</h4>
        <video controls muted autoplay loop playsinline style="max-width: 100%; border-radius: 8px;">
          <source src="./static/videos/bimanual_whisk.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-fullhd">
    <h2 class="title is-3 has-text-centered">Simulation Results</h2>

    <div class="content has-text-justified mb-5">
      <p>
        We evaluate ViPRA on the <b>SIMPLER</b> benchmark, a suite of simulated tasks built to test generalist robot control across diverse goals and object types. ViPRA outperforms all prior baselines in both discrete and continuous control settings.
      </p>

      <p>
        <p>
          In simulation, our discrete variant ViPRA-AR achieves the highest success rate—showing that discretized action prediction in an autoregressive manner can be very effective in low-noise, fully observed environments. Our continuous model, ViPRA-FM, also outperforms all other baselines, both discrete and continuous, highlighting the strength of our video-based pretraining. Its slightly lower score compared to ViPRA-AR reflects the slower convergence typical of flow matching. ViPRA-FM is better suited for real-world deployment, where smoothness and robustness are critical.
        </p>
        
      </p>
      
      
      
    </div>

    <div class="has-text-centered mb-5">
      <img src="./static/images/sim_results.png" alt="SIMPLER Results Chart" style="max-width: 100%; border-radius: 8px;">
      <figcaption class="has-text-grey has-text-justified mt-2">
        <b>Simulation Results:</b> Success and grasp rates across four simulated tasks in the SIMPLER benchmark. We compare ViPRA with discrete-action baselines like Scratch-AR, OpenVLA, and LAPA (which also uses latent action pretraining, but without video prediction), as well as continuous-control models such as Pi-0 and Scratch-FM.
      </figcaption>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-fullhd">
    <h2 class="title is-3 has-text-centered">Acknowledgements</h2>
    <p>
      We thank Jason Liu and Tony Tao for their assistance in conducting the robot experiments, and Jim Yang, Mohan Kumar Srirama, and Tal Daniel for helpful discussions and feedback. This work was supported in part by the Air Force Office of Scientific Research (AFOSR)
      under Grant No.~FA9550-23-1-0747 and by the Office of Naval Research (ONR) MURI
      under Grant No.~N00014-24-1-2748.
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-fullhd">
    <h2 class="title is-3 has-text-centered">Citation</h2>
    <pre><code>@inproceedings{routray2025vipra,
  title        = {ViPRA: Video Prediction for Robot Actions},
  author       = {Routray, Sandeep and Pan, Hengkai and Jain, Unnat and Bahl, Shikhar and Pathak, Deepak},
  booktitle    = {NeurIPS 2025 Workshop on Embodied World Models for Decision Making},
  year         = {2025},
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
